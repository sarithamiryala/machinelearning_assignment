{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53caa765",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. In a linear equation, what is the difference between a dependent variable and an independent variable?\n",
    "Ans->Dependent variable is one whose values are dependent on the values of independent variable. In other words, the independent variable's values dictate the values of the dependent variable. In a linear equation, this would mean that the dependent variable would be in a linear relationship with the dependent variable.\n",
    "\n",
    "2. What is the concept of simple linear regression? Give a specific example.\n",
    "Ans->The main concept is to make a line fit data points of highly correlated pair of variables such that the root mean square error value is the least. This would mean that we need to make sure that the distance between the fitted regression line and the data points(residuals) is least. Several assumptions are made to make simple linear regression work. Linearity, Independence of variables involved, residuals must be normally distributed are assumptions.\n",
    "\n",
    "3. In a linear regression, define the slope.Â¶\n",
    "Ans->The numerical property of a line that expresses the direction of the line and steepness of the line is called the slope. Slope can be calculated as\n",
    "\n",
    "m=(y2-y1)/(x2-x1)\n",
    "\n",
    "Slope is important in linear regression as it establishes the relationship(negatively linear or positively linear) between feature and the target variable.\n",
    "\n",
    "4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2).\n",
    "Ans->Slope would be 3/2 or 1.5\n",
    "\n",
    "5. In linear regression, what are the conditions for a positive slope?\n",
    "Ans->For a positive slope, the two linearly related variables must have a positive correlation coefficient. (positive slope)\n",
    "\n",
    "6. In linear regression, what are the conditions for a negative slope?\n",
    "Ans->For a positive slope, the two linearly related variables must have a negative correlation coefficient. (negative slope)\n",
    "\n",
    "7. What is multiple linear regression and how does it work?\n",
    "Ans->Multiple linear regression is an extension of the concept of simple linear regression where a single dependent variable is linearly correlated and is in linear relationship with multiple independent variables.\n",
    "\n",
    "8. In multiple linear regression, define the number of squares due to error.\n",
    "Ans->Sum of squares (SS) is a statistical tool that is used to identify the dispersion of data as well as how well the data can fit the model in regression analysis. The sum of squares got its name because it is calculated by finding the sum of the squared differences.\n",
    "\n",
    "9. In multiple linear regression, define the number of squares due to regression.\n",
    "Ans->Regression sum of squares (also known as the sum of squares due to regression or explained sum of squares) The regression sum of squares describes how well a regression model represents the modeled data. A higher regression sum of squares indicates that the model does not fit the data well.\n",
    "\n",
    "10.In a regression equation, what is multicollinearity?\n",
    "ans->In regression, \"multicollinearity\" refers to predictors that are correlated with other predictors. Multicollinearity occurs when our model includes multiple factors that are correlated not just to our response variable, but also to each other. In other words, it results when we have factors that are a bit redundant.\n",
    "\n",
    "11. What is heteroskedasticity, and what does it mean?\n",
    "Ans-> Heteroskedasticity (or heteroscedasticity) happens when the standard deviations of a predicted variable, monitored over different values of an independent variable or as related to prior time periods, are non-constant. With heteroskedasticity, the tell-tale sign upon visual inspection of the residual errors is that they will tend to fan out over time\n",
    "\n",
    "12. Describe the concept of ridge regression.\n",
    "Ans->Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.\n",
    "\n",
    "13. Describe the concept of lasso regression.\n",
    "Ans->Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\n",
    "\n",
    "14. What is polynomial regression and how does it work?\n",
    "Ans->Polynomial regression models can bend. They can be constructed to the nth-degree to minimize squared error and maximize rsquared. Depending on the nth degree, the line of best fit can have more or less curves. The higher the exponent, the more numerous the curves. y=m0+m1x^1+m2x^2 Here y:dependent variable m0 is the constant mi is the slope and x^i is the dependent variable and so on\n",
    "\n",
    "15. Describe the basis function.\n",
    "Ans->A basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.This is a generalization of linear regression that essentially replaces each input with a function of the input. (A linear basis function model that uses the identity function is just linear regression.\n",
    "\n",
    "16. Describe how logistic regression works.\n",
    "Ans->Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution. log(p/1-p) is the link function. Logarithmic transformation on the outcome variable allows us to model a non-linear association in a linear way. This is the equation used in Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
